{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    input_paths =  ['2023-wimbledon-1301-player1.csv',\n",
    "                    '2023-wimbledon-1301-player2.csv',\n",
    "                    '2023-wimbledon-1302-player1.csv',\n",
    "                    '2023-wimbledon-1302-player2.csv',\n",
    "                    '2023-wimbledon-1303-player1.csv',\n",
    "                    '2023-wimbledon-1303-player2.csv',\n",
    "                    '2023-wimbledon-1304-player1.csv',\n",
    "                    '2023-wimbledon-1304-player2.csv',\n",
    "                    '2023-wimbledon-1305-player1.csv',\n",
    "                    '2023-wimbledon-1305-player2.csv',\n",
    "                    '2023-wimbledon-1306-player1.csv',\n",
    "                    '2023-wimbledon-1306-player2.csv',\n",
    "                    '2023-wimbledon-1307-player1.csv',\n",
    "                    '2023-wimbledon-1307-player2.csv',\n",
    "                    '2023-wimbledon-1308-player1.csv',\n",
    "                    '2023-wimbledon-1308-player2.csv',\n",
    "                    '2023-wimbledon-1309-player1.csv',\n",
    "                    '2023-wimbledon-1309-player2.csv',\n",
    "                    '2023-wimbledon-1310-player1.csv',\n",
    "                    '2023-wimbledon-1310-player2.csv',\n",
    "                    '2023-wimbledon-1311-player1.csv',\n",
    "                    '2023-wimbledon-1311-player2.csv',\n",
    "                    '2023-wimbledon-1312-player1.csv',\n",
    "                    '2023-wimbledon-1312-player2.csv',\n",
    "                    '2023-wimbledon-1313-player1.csv',\n",
    "                    '2023-wimbledon-1313-player2.csv',\n",
    "                    '2023-wimbledon-1314-player1.csv',\n",
    "                    '2023-wimbledon-1314-player2.csv',\n",
    "                    '2023-wimbledon-1315-player1.csv',\n",
    "                    '2023-wimbledon-1315-player2.csv',\n",
    "                    '2023-wimbledon-1316-player1.csv',\n",
    "                    '2023-wimbledon-1316-player2.csv',\n",
    "                    '2023-wimbledon-1401-player1.csv',\n",
    "                    '2023-wimbledon-1401-player2.csv',\n",
    "                    '2023-wimbledon-1402-player1.csv',\n",
    "                    '2023-wimbledon-1402-player2.csv',\n",
    "                    '2023-wimbledon-1403-player1.csv',\n",
    "                    '2023-wimbledon-1403-player2.csv',\n",
    "                    '2023-wimbledon-1404-player1.csv',\n",
    "                    '2023-wimbledon-1404-player2.csv',\n",
    "                    '2023-wimbledon-1405-player1.csv',\n",
    "                    '2023-wimbledon-1405-player2.csv',\n",
    "                    '2023-wimbledon-1406-player1.csv',\n",
    "                    '2023-wimbledon-1406-player2.csv',\n",
    "                    '2023-wimbledon-1407-player1.csv',\n",
    "                    '2023-wimbledon-1407-player2.csv',\n",
    "                    '2023-wimbledon-1408-player1.csv',\n",
    "                    '2023-wimbledon-1408-player2.csv',\n",
    "                    '2023-wimbledon-1501-player1.csv',\n",
    "                    '2023-wimbledon-1501-player2.csv',\n",
    "                    '2023-wimbledon-1502-player1.csv',\n",
    "                    '2023-wimbledon-1502-player2.csv',\n",
    "                    '2023-wimbledon-1503-player1.csv',\n",
    "                    '2023-wimbledon-1503-player2.csv',\n",
    "                    '2023-wimbledon-1504-player1.csv',\n",
    "                    '2023-wimbledon-1504-player2.csv',\n",
    "                    '2023-wimbledon-1601-player1.csv',\n",
    "                    '2023-wimbledon-1601-player2.csv',\n",
    "                    '2023-wimbledon-1602-player1.csv',\n",
    "                    '2023-wimbledon-1602-player2.csv',\n",
    "                    '2023-wimbledon-1701-player1.csv',\n",
    "                    '2023-wimbledon-1701-player2.csv',\n",
    "                    ]\n",
    "    timestep = 10\n",
    "    batch_size = 64\n",
    "    feature_size = 12#16\n",
    "    hidden_size = 60 # TODO: can change\n",
    "    output_size = 4\n",
    "    predict_len = 5\n",
    "    num_layers = 2 # TODO: can change\n",
    "    epochs = 100 # TODO: can change\n",
    "    best_loss = 100\n",
    "    learning_rate = 0.001 # TODO: can change\n",
    "    model_name = 'GRU_for_tennis_momentum_swing'\n",
    "    save_path = './trained_models/{}.pth'.format(model_name)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def delete_columns(input_file, output_file, columns_to_delete):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            # Exclude columns to delete (using 0-based index)\n",
    "            filtered_row = [value for idx, value in enumerate(row) if idx not in columns_to_delete]\n",
    "            writer.writerow(filtered_row)\n",
    "\n",
    "# Example usage:\n",
    "input_csv_file = config.input_paths[0]\n",
    "output_csv_file = 'your_output_file.csv'\n",
    "columns_to_delete = [10, 11, 14, 15]  # Columns are 0-based, so 11 becomes 10, 12 becomes 11, and so on.\n",
    "for input_path in config.input_paths:\n",
    "    delete_columns('./data_for_training/data/'+input_path, './data_for_training/data1/'+input_path, columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match 1: point 10-237 in train set (for TWO times, one for each player), point 238-294 in test set (for TWO times, one for each player).\n",
      "match 2: point 10-158 in train set (for TWO times, one for each player), point 159-195 in test set (for TWO times, one for each player).\n",
      "match 3: point 10-104 in train set (for TWO times, one for each player), point 105-128 in test set (for TWO times, one for each player).\n",
      "match 4: point 10-267 in train set (for TWO times, one for each player), point 268-331 in test set (for TWO times, one for each player).\n",
      "match 5: point 10-194 in train set (for TWO times, one for each player), point 195-240 in test set (for TWO times, one for each player).\n",
      "match 6: point 10-263 in train set (for TWO times, one for each player), point 264-326 in test set (for TWO times, one for each player).\n",
      "match 7: point 10-183 in train set (for TWO times, one for each player), point 184-226 in test set (for TWO times, one for each player).\n",
      "match 8: point 10-149 in train set (for TWO times, one for each player), point 150-184 in test set (for TWO times, one for each player).\n",
      "match 9: point 10-167 in train set (for TWO times, one for each player), point 168-207 in test set (for TWO times, one for each player).\n",
      "match 10: point 10-251 in train set (for TWO times, one for each player), point 252-312 in test set (for TWO times, one for each player).\n",
      "match 11: point 10-133 in train set (for TWO times, one for each player), point 134-164 in test set (for TWO times, one for each player).\n",
      "match 12: point 10-217 in train set (for TWO times, one for each player), point 218-269 in test set (for TWO times, one for each player).\n",
      "match 13: point 10-229 in train set (for TWO times, one for each player), point 230-284 in test set (for TWO times, one for each player).\n",
      "match 14: point 10-145 in train set (for TWO times, one for each player), point 146-179 in test set (for TWO times, one for each player).\n",
      "match 15: point 10-155 in train set (for TWO times, one for each player), point 156-192 in test set (for TWO times, one for each player).\n",
      "match 16: point 10-131 in train set (for TWO times, one for each player), point 132-161 in test set (for TWO times, one for each player).\n",
      "match 17: point 10-177 in train set (for TWO times, one for each player), point 178-219 in test set (for TWO times, one for each player).\n",
      "match 18: point 10-216 in train set (for TWO times, one for each player), point 217-268 in test set (for TWO times, one for each player).\n",
      "match 19: point 10-95 in train set (for TWO times, one for each player), point 96-116 in test set (for TWO times, one for each player).\n",
      "match 20: point 10-226 in train set (for TWO times, one for each player), point 227-280 in test set (for TWO times, one for each player).\n",
      "match 21: point 10-169 in train set (for TWO times, one for each player), point 170-209 in test set (for TWO times, one for each player).\n",
      "match 22: point 10-153 in train set (for TWO times, one for each player), point 154-189 in test set (for TWO times, one for each player).\n",
      "match 23: point 10-262 in train set (for TWO times, one for each player), point 263-325 in test set (for TWO times, one for each player).\n",
      "match 24: point 10-214 in train set (for TWO times, one for each player), point 215-265 in test set (for TWO times, one for each player).\n",
      "match 25: point 10-148 in train set (for TWO times, one for each player), point 149-183 in test set (for TWO times, one for each player).\n",
      "match 26: point 10-223 in train set (for TWO times, one for each player), point 224-277 in test set (for TWO times, one for each player).\n",
      "match 27: point 10-151 in train set (for TWO times, one for each player), point 152-187 in test set (for TWO times, one for each player).\n",
      "match 28: point 10-171 in train set (for TWO times, one for each player), point 172-211 in test set (for TWO times, one for each player).\n",
      "match 29: point 10-124 in train set (for TWO times, one for each player), point 125-153 in test set (for TWO times, one for each player).\n",
      "match 30: point 10-159 in train set (for TWO times, one for each player), point 160-196 in test set (for TWO times, one for each player).\n",
      "match 31: point 10-264 in train set (for TWO times, one for each player), point 265-328 in test set (for TWO times, one for each player).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_data(dataX, dataY, flag, timestep, feature_size, output_size, pred_len):\n",
    "    myDataX = []\n",
    "    myDataY = []\n",
    "    for i in range(0, len(dataX) - timestep):\n",
    "        target_row = dataY[i + timestep - 1].reshape(-1, output_size)\n",
    "        res = []\n",
    "        for j in range(pred_len):\n",
    "            for k in range(output_size):\n",
    "                if target_row[j][k] == 1:\n",
    "                    if flag:\n",
    "                        res.append(2 - k if k % 2 == 0 else 4 - k)\n",
    "                    else:\n",
    "                        res.append(k)\n",
    "                    break\n",
    "        if len(res) != pred_len:\n",
    "            break\n",
    "        myDataX.append(dataX[i: i + timestep])\n",
    "        myDataY.append(res)\n",
    "    myDataX = np.array(myDataX)\n",
    "    myDataY = np.array(myDataY)\n",
    "\n",
    "    train_size = int(np.round(0.8 * myDataX.shape[0]))\n",
    "    trainX = myDataX[:train_size, :].reshape(-1, timestep, feature_size)\n",
    "    testX = myDataX[train_size:, :].reshape(-1, timestep, feature_size)\n",
    "    trainY = myDataY[:train_size, :].reshape(-1, pred_len)\n",
    "    testY = myDataY[train_size:, :].reshape(-1, pred_len)\n",
    "    return (trainX, trainY, testX, testY)\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "testX = []\n",
    "testY = []\n",
    "dfy = pd.read_csv('./data_for_training/momentum_condition.csv')\n",
    "dataY = np.array(dfy)\n",
    "df_idx = pd.read_csv('./data_for_training/match_index.csv')\n",
    "match_id = np.array(df_idx).reshape(-1)\n",
    "for i in range(len(match_id)):\n",
    "    match_id[i] -= 1\n",
    "pos = 0\n",
    "match_no = 0\n",
    "\n",
    "for input_path in config.input_paths:\n",
    "    input_path = './data_for_training/data1/' + input_path\n",
    "    dfx = pd.read_csv(input_path)\n",
    "    dataX = np.array(dfx)\n",
    "    a, b, c, d = fetch_data(dataX, dataY[pos: match_id[match_no // 2]], match_no % 2, config.timestep, config.feature_size, config.output_size, config.predict_len)\n",
    "    if match_no % 2 == 1:\n",
    "        pos = match_id[match_no // 2]\n",
    "        print(\"match {:d}: point {:d}-{:d} in train set (for TWO times, one for each player), point {:d}-{:d} in test set (for TWO times, one for each player).\".format(match_no // 2 + 1, config.timestep, config.timestep + len(a) - 1, config.timestep + len(a), config.timestep + len(a) + len(c) - 1))\n",
    "    match_no += 1\n",
    "    for x in a:\n",
    "        trainX.append(x)\n",
    "    for y in b:\n",
    "        trainY.append(y)\n",
    "    for x in c:\n",
    "        testX.append(x)\n",
    "    for y in d:\n",
    "        testY.append(y)\n",
    "trainX = np.array(trainX).reshape(-1, config.timestep, config.feature_size)\n",
    "testX = np.array(testX).reshape(-1, config.timestep, config.feature_size)\n",
    "trainY = np.array(trainY).reshape(-1, config.predict_len)\n",
    "testY = np.array(testY).reshape(-1, config.predict_len)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(trainX).to(torch.float32)\n",
    "y_train_tensor = torch.from_numpy(trainY).to(torch.long)\n",
    "\n",
    "x_test_tensor = torch.from_numpy(testX).to(torch.float32)\n",
    "y_test_tensor = torch.from_numpy(testY).to(torch.long)\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, config.batch_size, False)\n",
    "test_loader = DataLoader(test_data, config.batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "class GRURNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc4 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc5 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, input_seq):\n",
    "        batch_size = input_seq.shape[0]\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        output, _ = self.gru(input_seq,h_0)\n",
    "        pred1 = self.fc1(output)\n",
    "        pred2 = self.fc2(output)\n",
    "        pred3 = self.fc3(output)\n",
    "        pred4 = self.fc4(output)\n",
    "        pred5 = self.fc5(output)\n",
    "        pred1, pred2, pred3, pred4, pred5 = pred1[:, -1, :], pred2[:, -1, :], pred3[:, -1, :], pred4[:, -1, :], pred5[:, -1, :]\n",
    "        pred1, pred2, pred3, pred4, pred5 = self.softmax(pred1), self.softmax(pred2), self.softmax(pred3), self.softmax(pred4), self.softmax(pred5)\n",
    "        pred = torch.stack([pred1, pred2, pred3, pred4, pred5], dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/100] loss: 3.981: 100%|██████████| 171/171 [00:01<00:00, 118.53it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.84it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 113.36it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 128.88it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.66it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.14it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 126.53it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 120.57it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 132.07it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.46it/s]\n",
      "train epoch[11/100] loss: 3.935: 100%|██████████| 171/171 [00:01<00:00, 134.58it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.67it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.48it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.14it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.82it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.03it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.14it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.56it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.95it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 132.35it/s]\n",
      "train epoch[21/100] loss: 3.976: 100%|██████████| 171/171 [00:01<00:00, 121.37it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.75it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.32it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.37it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 126.48it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.13it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.07it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 120.62it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.37it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.05it/s]\n",
      "train epoch[31/100] loss: 3.894: 100%|██████████| 171/171 [00:01<00:00, 122.65it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.31it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.03it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.35it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 123.78it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.81it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.76it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.74it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 122.08it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.72it/s]\n",
      "train epoch[41/100] loss: 3.900: 100%|██████████| 171/171 [00:01<00:00, 137.83it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.10it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.85it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.77it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.15it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.25it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.31it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 140.65it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 140.75it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.05it/s]\n",
      "train epoch[51/100] loss: 3.878: 100%|██████████| 171/171 [00:01<00:00, 118.40it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 118.89it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 126.98it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.06it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 120.78it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.58it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.59it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 127.52it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.09it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.73it/s]\n",
      "train epoch[61/100] loss: 3.924: 100%|██████████| 171/171 [00:01<00:00, 136.22it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.25it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.93it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.71it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.89it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.74it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 140.47it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.92it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.12it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 135.16it/s]\n",
      "train epoch[71/100] loss: 3.854: 100%|██████████| 171/171 [00:01<00:00, 137.91it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 131.81it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 132.10it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.39it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.69it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.53it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.25it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.23it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 139.26it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.01it/s]\n",
      "train epoch[81/100] loss: 3.845: 100%|██████████| 171/171 [00:01<00:00, 130.16it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 138.83it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.90it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 131.29it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 128.37it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 126.41it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.84it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 133.54it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 136.23it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.39it/s]\n",
      "train epoch[91/100] loss: 3.827: 100%|██████████| 171/171 [00:01<00:00, 138.22it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.68it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.59it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.40it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.57it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.30it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.97it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.00it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 137.55it/s]\n",
      "100%|██████████| 171/171 [00:01<00:00, 134.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for data in train_bar:\n",
    "        x_train, y_train = data\n",
    "        optimizer.zero_grad()\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        loss1 = loss_function(y_train_pred[:, 0, :], y_train[:, 0])\n",
    "        loss2 = loss_function(y_train_pred[:, 1, :], y_train[:, 1])\n",
    "        loss3 = loss_function(y_train_pred[:, 2, :], y_train[:, 2])\n",
    "        loss4 = loss_function(y_train_pred[:, 3, :], y_train[:, 3])\n",
    "        loss5 = loss_function(y_train_pred[:, 4, :], y_train[:, 4])\n",
    "        # loss = max(loss1, loss2, loss3, loss4, loss5)\n",
    "        loss = loss1 + loss2 + loss3 + loss4 + loss5\n",
    "        # loss = 5 * loss1 + 2 * loss2 + loss3 + loss4 + loss5\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"loss =\", loss)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if epoch % 10 == 0:\n",
    "            train_bar.desc = 'train epoch[{}/{}] loss: {:.3f}'.format(epoch + 1, config.epochs, loss)\n",
    "    \n",
    "    # model.eval()\n",
    "    # test_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     test_bar = tqdm(test_loader)\n",
    "    #     for data in test_bar:\n",
    "    #         x_test, y_test = data\n",
    "    #         y_test_pred = model(x_test)\n",
    "    #         loss1 = loss_function(y_test_pred[:, 0, :], y_test[:, 0])\n",
    "    #         loss2 = loss_function(y_test_pred[:, 1, :], y_test[:, 1])\n",
    "    #         loss3 = loss_function(y_test_pred[:, 2, :], y_test[:, 2])\n",
    "    #         loss4 = loss_function(y_test_pred[:, 3, :], y_test[:, 3])\n",
    "    #         loss5 = loss_function(y_test_pred[:, 4, :], y_test[:, 4])\n",
    "    #         # test_loss = max(loss1, loss2, loss3, loss4, loss5)\n",
    "    #         # test_loss = loss1 + loss2 + loss3 + loss4 + loss5\n",
    "    #         test_loss = 5 * loss1 + 3 * loss2 + loss3 + loss4 + loss5\n",
    "    \n",
    "    # if test_loss < config.best_loss:\n",
    "    #     config.best_loss = test_loss\n",
    "    #     torch.save(model.state_dict(), config.save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:02<00:00, 72.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set predict rate for step 1 is: 0.832661\n",
      "train set predict rate for step 2 is: 0.862995\n",
      "train set predict rate for step 3 is: 0.863636\n",
      "train set predict rate for step 4 is: 0.856305\n",
      "train set predict rate for step 5 is: 0.826521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 77.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set predict rate for step 1 is: 0.362069\n",
      "test set predict rate for step 2 is: 0.347395\n",
      "test set predict rate for step 3 is: 0.341159\n",
      "test set predict rate for step 4 is: 0.324285\n",
      "test set predict rate for step 5 is: 0.305209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "# model.load_state_dict(torch.load(config.save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_bar = tqdm(train_loader)\n",
    "\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    for data in train_bar:\n",
    "        x_train, y_train = data\n",
    "        y_train_pred = model(x_train)\n",
    "        for i in range(config.predict_len):\n",
    "            tot[i] += x_train.shape[0]\n",
    "        for i in range(x_train.shape[0]):\n",
    "            for j in range(config.predict_len):\n",
    "                pred = 0\n",
    "                for k in range(config.output_size):\n",
    "                    if y_train_pred[i][j][k].item() > y_train_pred[i][j][pred].item():\n",
    "                        pred = k\n",
    "                predict_rate[j] += (1 if pred == y_train[i][j] else 0)\n",
    "    for i in range(config.predict_len):\n",
    "        print('train set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "    test_bar = tqdm(test_loader)\n",
    "\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    for data in test_bar:\n",
    "        x_test, y_test = data\n",
    "        y_test_pred = model(x_test)\n",
    "        for i in range(config.predict_len):\n",
    "            tot[i] += x_test.shape[0]\n",
    "        for i in range(x_test.shape[0]):\n",
    "            for j in range(config.predict_len):\n",
    "                pred = 0\n",
    "                for k in range(config.output_size):\n",
    "                    if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                        pred = k\n",
    "                predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "    for i in range(config.predict_len):\n",
    "        print('test set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------model_sum------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:02<00:00, 69.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set predict rate for step 1 is: 0.832661\n",
      "train set predict rate for step 2 is: 0.862995\n",
      "train set predict rate for step 3 is: 0.863636\n",
      "train set predict rate for step 4 is: 0.856305\n",
      "train set predict rate for step 5 is: 0.826521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 76.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set predict rate for step 1 is: 0.362069\n",
      "test set predict rate for step 2 is: 0.347395\n",
      "test set predict rate for step 3 is: 0.341159\n",
      "test set predict rate for step 4 is: 0.324285\n",
      "test set predict rate for step 5 is: 0.305209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_prediction(model, output_file):\n",
    "    model.eval()\n",
    "    \n",
    "    lst = []\n",
    "    column = ['is_train_set', 'pred_y1', 'pred_y2', 'pred_y3', 'pred_y4', 'pred_y5', 'target_y1', 'target_y2', 'target_y3', 'target_y4', 'target_y5']\n",
    "    with torch.no_grad():\n",
    "        train_bar = tqdm(train_loader)\n",
    "\n",
    "        predict_rate = [0 for _ in range(config.predict_len)]\n",
    "        tot = [0 for _ in range(config.predict_len)]\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            y_train_pred = model(x_train)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_train.shape[0]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                row = [1]\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_train_pred[i][j][k].item() > y_train_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_train[i][j] else 0)\n",
    "                    row.append(pred)\n",
    "                for j in range(config.predict_len):\n",
    "                    row.append(y_train[i][j].item())\n",
    "                lst.append(row)\n",
    "        for i in range(config.predict_len):\n",
    "            print('train set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "        \n",
    "        test_bar = tqdm(test_loader)\n",
    "\n",
    "        predict_rate = [0 for _ in range(config.predict_len)]\n",
    "        tot = [0 for _ in range(config.predict_len)]\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            y_test_pred = model(x_test)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_test.shape[0]\n",
    "            for i in range(x_test.shape[0]):\n",
    "                row = [0]\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "                    row.append(pred)\n",
    "                for j in range(config.predict_len):\n",
    "                    row.append(y_test[i][j].item())\n",
    "                lst.append(row)\n",
    "        for i in range(config.predict_len):\n",
    "            print('test set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "    df = pd.DataFrame(lst, columns = column)\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "model_max = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "model_sum = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "model_lc = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "#model_max.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_max.pth'))\n",
    "model_sum.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_sum.pth'))\n",
    "#model_lc.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_lc.pth'))\n",
    "# print('-----------model_max------------')\n",
    "# compute_prediction(model_max, './pred_res/model_max.csv')\n",
    "print('-----------model_sum------------')\n",
    "compute_prediction(model_sum, './pred_res/model_sum.csv')\n",
    "# print('----model_linear_combination----')\n",
    "# compute_prediction(model_lc, './pred_res/model_lc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 74.05it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 78.08it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.23it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.70it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.76it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.86it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.74it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.80it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.46it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 74.61it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 77.57it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.79it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.79it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.61it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.81it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.61it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.46it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.29it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 77.54it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.18it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.79it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 73.44it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.86it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 78.42it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.72it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.73it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.27it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 73.23it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 72.81it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 75.56it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.15it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.75it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 78.94it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 68.92it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 78.45it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.96it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 80.07it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.18it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 76.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def calcPredictRate(model, test_loader):\n",
    "    model.eval()\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_bar = tqdm(test_loader)\n",
    "\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            y_test_pred = model(x_test)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_test.shape[0]\n",
    "            for i in range(x_test.shape[0]):\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "    return np.array([predict_rate[i] / tot[i] for i in range(config.predict_len)])\n",
    "\n",
    "def calcFeatureImportance(model):\n",
    "    accuracy = calcPredictRate(model, test_loader)\n",
    "    importance = np.zeros(config.feature_size)\n",
    "    for i in range(config.feature_size):\n",
    "        ntestX = testX.copy()\n",
    "        nfeature = ntestX[:, :, i].copy().reshape(-1)\n",
    "        np.random.shuffle(nfeature)\n",
    "        nfeature = nfeature.reshape(-1, config.timestep)\n",
    "        for j in range(ntestX.shape[0]):\n",
    "            for k in range(ntestX.shape[1]):\n",
    "                ntestX[j][k][i] = nfeature[j][k]\n",
    "        nx_test_tensor = torch.from_numpy(ntestX).to(torch.float32)\n",
    "        ntest_data = TensorDataset(nx_test_tensor, y_test_tensor)\n",
    "        ntest_loader = DataLoader(ntest_data, config.batch_size, False)\n",
    "        naccuracy = calcPredictRate(model, ntest_loader)\n",
    "        importance[i] = sum([(accuracy[j] - naccuracy[j]) * (accuracy[j] - naccuracy[j]) for j in range(config.predict_len)])\n",
    "\n",
    "    tot = np.sum(importance)\n",
    "    for i in range(importance.shape[0]):\n",
    "        importance[i] = importance[i] / tot\n",
    "    \n",
    "    return importance\n",
    "\n",
    "column_name = ['model_name'] + ['f' + str(i + 1) for i in range(config.feature_size)]\n",
    "z = [['model_sum'] + calcFeatureImportance(model_sum).tolist()]\n",
    "#z = [['model_max'] + calcFeatureImportance(model_max).tolist(), ['model_sum'] + calcFeatureImportance(model_sum).tolist(), ['model_lc'] + calcFeatureImportance(model_lc).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['model_max', 0.03239767528131539, 0.006924693953258602, 0.025472981328057566, 0.09521454185730235, 0.03734388524792863, 0.11969828119203615, 0.07394583900086583, 0.11215531099295152, 0.043650302955360126, 0.21342896005935477, 0.00420427847162107, 0.23556324965994793], ['model_sum', 0.03260437375745547, 0.0015904572564612953, 0.024652087475148996, 0.14035785288270317, 0.01033797216699782, 0.011928429423459234, 0.11848906560636122, 0.16103379721669997, 0.02266401590457286, 0.16620278330019966, 0.01153081510934397, 0.29860834990059637], ['model_lc', 0.03875968992248023, 0.0, 0.3139534883720935, 0.0038759689922481986, 0.050387596899224826, 0.0038759689922481986, 0.0038759689922481986, 0.0038759689922481986, 0.0038759689922481986, 0.0038759689922481986, 0.015503875968992794, 0.5581395348837195]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "17 columns passed, passed data had 13 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[1;32m   1022\u001b[0m \n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 17 columns passed, passed data had 13 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(z)\n\u001b[0;32m----> 2\u001b[0m importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mz, columns\u001b[38;5;241m=\u001b[39mcolumn_name)\n\u001b[1;32m      3\u001b[0m importance\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_models/feature_importance1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    749\u001b[0m         data,\n\u001b[1;32m    750\u001b[0m         columns,\n\u001b[1;32m    751\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    752\u001b[0m         dtype,\n\u001b[1;32m    753\u001b[0m     )\n\u001b[1;32m    754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    755\u001b[0m         arrays,\n\u001b[1;32m    756\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 510\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    511\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    872\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    873\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 875\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    975\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 17 columns passed, passed data had 13 columns"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "importance = pd.DataFrame(data=z, columns=column_name)\n",
    "importance.to_csv('./trained_models/feature_importance1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
