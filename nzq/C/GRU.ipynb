{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    input_paths =  ['2023-wimbledon-1301-player1.csv',\n",
    "                    '2023-wimbledon-1301-player2.csv',\n",
    "                    '2023-wimbledon-1302-player1.csv',\n",
    "                    '2023-wimbledon-1302-player2.csv',\n",
    "                    '2023-wimbledon-1303-player1.csv',\n",
    "                    '2023-wimbledon-1303-player2.csv',\n",
    "                    '2023-wimbledon-1304-player1.csv',\n",
    "                    '2023-wimbledon-1304-player2.csv',\n",
    "                    '2023-wimbledon-1305-player1.csv',\n",
    "                    '2023-wimbledon-1305-player2.csv',\n",
    "                    '2023-wimbledon-1306-player1.csv',\n",
    "                    '2023-wimbledon-1306-player2.csv',\n",
    "                    '2023-wimbledon-1307-player1.csv',\n",
    "                    '2023-wimbledon-1307-player2.csv',\n",
    "                    '2023-wimbledon-1308-player1.csv',\n",
    "                    '2023-wimbledon-1308-player2.csv',\n",
    "                    '2023-wimbledon-1309-player1.csv',\n",
    "                    '2023-wimbledon-1309-player2.csv',\n",
    "                    '2023-wimbledon-1310-player1.csv',\n",
    "                    '2023-wimbledon-1310-player2.csv',\n",
    "                    '2023-wimbledon-1311-player1.csv',\n",
    "                    '2023-wimbledon-1311-player2.csv',\n",
    "                    '2023-wimbledon-1312-player1.csv',\n",
    "                    '2023-wimbledon-1312-player2.csv',\n",
    "                    '2023-wimbledon-1313-player1.csv',\n",
    "                    '2023-wimbledon-1313-player2.csv',\n",
    "                    '2023-wimbledon-1314-player1.csv',\n",
    "                    '2023-wimbledon-1314-player2.csv',\n",
    "                    '2023-wimbledon-1315-player1.csv',\n",
    "                    '2023-wimbledon-1315-player2.csv',\n",
    "                    '2023-wimbledon-1316-player1.csv',\n",
    "                    '2023-wimbledon-1316-player2.csv',\n",
    "                    '2023-wimbledon-1401-player1.csv',\n",
    "                    '2023-wimbledon-1401-player2.csv',\n",
    "                    '2023-wimbledon-1402-player1.csv',\n",
    "                    '2023-wimbledon-1402-player2.csv',\n",
    "                    '2023-wimbledon-1403-player1.csv',\n",
    "                    '2023-wimbledon-1403-player2.csv',\n",
    "                    '2023-wimbledon-1404-player1.csv',\n",
    "                    '2023-wimbledon-1404-player2.csv',\n",
    "                    '2023-wimbledon-1405-player1.csv',\n",
    "                    '2023-wimbledon-1405-player2.csv',\n",
    "                    '2023-wimbledon-1406-player1.csv',\n",
    "                    '2023-wimbledon-1406-player2.csv',\n",
    "                    '2023-wimbledon-1407-player1.csv',\n",
    "                    '2023-wimbledon-1407-player2.csv',\n",
    "                    '2023-wimbledon-1408-player1.csv',\n",
    "                    '2023-wimbledon-1408-player2.csv',\n",
    "                    '2023-wimbledon-1501-player1.csv',\n",
    "                    '2023-wimbledon-1501-player2.csv',\n",
    "                    '2023-wimbledon-1502-player1.csv',\n",
    "                    '2023-wimbledon-1502-player2.csv',\n",
    "                    '2023-wimbledon-1503-player1.csv',\n",
    "                    '2023-wimbledon-1503-player2.csv',\n",
    "                    '2023-wimbledon-1504-player1.csv',\n",
    "                    '2023-wimbledon-1504-player2.csv',\n",
    "                    '2023-wimbledon-1601-player1.csv',\n",
    "                    '2023-wimbledon-1601-player2.csv',\n",
    "                    '2023-wimbledon-1602-player1.csv',\n",
    "                    '2023-wimbledon-1602-player2.csv',\n",
    "                    '2023-wimbledon-1701-player1.csv',\n",
    "                    '2023-wimbledon-1701-player2.csv',\n",
    "                    ]\n",
    "    timestep = 10\n",
    "    batch_size = 64\n",
    "    feature_size = 12#16\n",
    "    hidden_size = 60 # TODO: can change\n",
    "    output_size = 4\n",
    "    predict_len = 5\n",
    "    num_layers = 2 # TODO: can change\n",
    "    epochs = 600 # TODO: can change\n",
    "    best_loss = 100\n",
    "    learning_rate = 0.001 # TODO: can change\n",
    "    model_name = 'GRU_for_tennis_momentum_swing'\n",
    "    save_path = './trained_models/{}.pth'.format(model_name)\n",
    "    loss_type = 2\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def delete_columns(input_file, output_file, columns_to_delete):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            # Exclude columns to delete (using 0-based index)\n",
    "            filtered_row = [value for idx, value in enumerate(row) if idx not in columns_to_delete]\n",
    "            writer.writerow(filtered_row)\n",
    "\n",
    "# Example usage:\n",
    "input_csv_file = config.input_paths[0]\n",
    "output_csv_file = 'your_output_file.csv'\n",
    "columns_to_delete = [10, 11, 14, 15]  # Columns are 0-based, so 11 becomes 10, 12 becomes 11, and so on.\n",
    "for input_path in config.input_paths:\n",
    "    delete_columns('./data_for_training/data/'+input_path, './data_for_training/data1/'+input_path, columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match 1: point 10-237 in train set (for TWO times, one for each player), point 238-294 in test set (for TWO times, one for each player).\n",
      "match 2: point 10-158 in train set (for TWO times, one for each player), point 159-195 in test set (for TWO times, one for each player).\n",
      "match 3: point 10-104 in train set (for TWO times, one for each player), point 105-128 in test set (for TWO times, one for each player).\n",
      "match 4: point 10-267 in train set (for TWO times, one for each player), point 268-331 in test set (for TWO times, one for each player).\n",
      "match 5: point 10-194 in train set (for TWO times, one for each player), point 195-240 in test set (for TWO times, one for each player).\n",
      "match 6: point 10-263 in train set (for TWO times, one for each player), point 264-326 in test set (for TWO times, one for each player).\n",
      "match 7: point 10-183 in train set (for TWO times, one for each player), point 184-226 in test set (for TWO times, one for each player).\n",
      "match 8: point 10-149 in train set (for TWO times, one for each player), point 150-184 in test set (for TWO times, one for each player).\n",
      "match 9: point 10-167 in train set (for TWO times, one for each player), point 168-207 in test set (for TWO times, one for each player).\n",
      "match 10: point 10-251 in train set (for TWO times, one for each player), point 252-312 in test set (for TWO times, one for each player).\n",
      "match 11: point 10-133 in train set (for TWO times, one for each player), point 134-164 in test set (for TWO times, one for each player).\n",
      "match 12: point 10-217 in train set (for TWO times, one for each player), point 218-269 in test set (for TWO times, one for each player).\n",
      "match 13: point 10-229 in train set (for TWO times, one for each player), point 230-284 in test set (for TWO times, one for each player).\n",
      "match 14: point 10-145 in train set (for TWO times, one for each player), point 146-179 in test set (for TWO times, one for each player).\n",
      "match 15: point 10-155 in train set (for TWO times, one for each player), point 156-192 in test set (for TWO times, one for each player).\n",
      "match 16: point 10-131 in train set (for TWO times, one for each player), point 132-161 in test set (for TWO times, one for each player).\n",
      "match 17: point 10-177 in train set (for TWO times, one for each player), point 178-219 in test set (for TWO times, one for each player).\n",
      "match 18: point 10-216 in train set (for TWO times, one for each player), point 217-268 in test set (for TWO times, one for each player).\n",
      "match 19: point 10-95 in train set (for TWO times, one for each player), point 96-116 in test set (for TWO times, one for each player).\n",
      "match 20: point 10-226 in train set (for TWO times, one for each player), point 227-280 in test set (for TWO times, one for each player).\n",
      "match 21: point 10-169 in train set (for TWO times, one for each player), point 170-209 in test set (for TWO times, one for each player).\n",
      "match 22: point 10-153 in train set (for TWO times, one for each player), point 154-189 in test set (for TWO times, one for each player).\n",
      "match 23: point 10-262 in train set (for TWO times, one for each player), point 263-325 in test set (for TWO times, one for each player).\n",
      "match 24: point 10-214 in train set (for TWO times, one for each player), point 215-265 in test set (for TWO times, one for each player).\n",
      "match 25: point 10-148 in train set (for TWO times, one for each player), point 149-183 in test set (for TWO times, one for each player).\n",
      "match 26: point 10-223 in train set (for TWO times, one for each player), point 224-277 in test set (for TWO times, one for each player).\n",
      "match 27: point 10-151 in train set (for TWO times, one for each player), point 152-187 in test set (for TWO times, one for each player).\n",
      "match 28: point 10-171 in train set (for TWO times, one for each player), point 172-211 in test set (for TWO times, one for each player).\n",
      "match 29: point 10-124 in train set (for TWO times, one for each player), point 125-153 in test set (for TWO times, one for each player).\n",
      "match 30: point 10-159 in train set (for TWO times, one for each player), point 160-196 in test set (for TWO times, one for each player).\n",
      "match 31: point 10-264 in train set (for TWO times, one for each player), point 265-328 in test set (for TWO times, one for each player).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_data(dataX, dataY, flag, timestep, feature_size, output_size, pred_len):\n",
    "    myDataX = []\n",
    "    myDataY = []\n",
    "    for i in range(0, len(dataX) - timestep):\n",
    "        target_row = dataY[i + timestep - 1].reshape(-1, output_size)\n",
    "        res = []\n",
    "        for j in range(pred_len):\n",
    "            for k in range(output_size):\n",
    "                if target_row[j][k] == 1:\n",
    "                    if flag:\n",
    "                        res.append(2 - k if k % 2 == 0 else 4 - k)\n",
    "                    else:\n",
    "                        res.append(k)\n",
    "                    break\n",
    "        if len(res) != pred_len:\n",
    "            break\n",
    "        myDataX.append(dataX[i: i + timestep])\n",
    "        myDataY.append(res)\n",
    "    myDataX = np.array(myDataX)\n",
    "    myDataY = np.array(myDataY)\n",
    "\n",
    "    train_size = int(np.round(0.8 * myDataX.shape[0]))\n",
    "    trainX = myDataX[:train_size, :].reshape(-1, timestep, feature_size)\n",
    "    testX = myDataX[train_size:, :].reshape(-1, timestep, feature_size)\n",
    "    trainY = myDataY[:train_size, :].reshape(-1, pred_len)\n",
    "    testY = myDataY[train_size:, :].reshape(-1, pred_len)\n",
    "    return (trainX, trainY, testX, testY)\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "testX = []\n",
    "testY = []\n",
    "dfy = pd.read_csv('./data_for_training/momentum_condition.csv')\n",
    "dataY = np.array(dfy)\n",
    "df_idx = pd.read_csv('./data_for_training/match_index.csv')\n",
    "match_id = np.array(df_idx).reshape(-1)\n",
    "for i in range(len(match_id)):\n",
    "    match_id[i] -= 1\n",
    "pos = 0\n",
    "match_no = 0\n",
    "\n",
    "for input_path in config.input_paths:\n",
    "    input_path = './data_for_training/data1/' + input_path\n",
    "    dfx = pd.read_csv(input_path)\n",
    "    dataX = np.array(dfx)\n",
    "    a, b, c, d = fetch_data(dataX, dataY[pos: match_id[match_no // 2]], match_no % 2, config.timestep, config.feature_size, config.output_size, config.predict_len)\n",
    "    if match_no % 2 == 1:\n",
    "        pos = match_id[match_no // 2]\n",
    "        print(\"match {:d}: point {:d}-{:d} in train set (for TWO times, one for each player), point {:d}-{:d} in test set (for TWO times, one for each player).\".format(match_no // 2 + 1, config.timestep, config.timestep + len(a) - 1, config.timestep + len(a), config.timestep + len(a) + len(c) - 1))\n",
    "    match_no += 1\n",
    "    for x in a:\n",
    "        trainX.append(x)\n",
    "    for y in b:\n",
    "        trainY.append(y)\n",
    "    for x in c:\n",
    "        testX.append(x)\n",
    "    for y in d:\n",
    "        testY.append(y)\n",
    "trainX = np.array(trainX).reshape(-1, config.timestep, config.feature_size)\n",
    "testX = np.array(testX).reshape(-1, config.timestep, config.feature_size)\n",
    "trainY = np.array(trainY).reshape(-1, config.predict_len)\n",
    "testY = np.array(testY).reshape(-1, config.predict_len)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(trainX).to(torch.float32)\n",
    "y_train_tensor = torch.from_numpy(trainY).to(torch.long)\n",
    "\n",
    "x_test_tensor = torch.from_numpy(testX).to(torch.float32)\n",
    "y_test_tensor = torch.from_numpy(testY).to(torch.long)\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, config.batch_size, False)\n",
    "test_loader = DataLoader(test_data, config.batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "class GRURNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc4 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.fc5 = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, input_seq):\n",
    "        batch_size = input_seq.shape[0]\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        output, _ = self.gru(input_seq,h_0)\n",
    "        pred1 = self.fc1(output)\n",
    "        pred2 = self.fc2(output)\n",
    "        pred3 = self.fc3(output)\n",
    "        pred4 = self.fc4(output)\n",
    "        pred5 = self.fc5(output)\n",
    "        pred1, pred2, pred3, pred4, pred5 = pred1[:, -1, :], pred2[:, -1, :], pred3[:, -1, :], pred4[:, -1, :], pred5[:, -1, :]\n",
    "        pred1, pred2, pred3, pred4, pred5 = self.softmax(pred1), self.softmax(pred2), self.softmax(pred3), self.softmax(pred4), self.softmax(pred5)\n",
    "        pred = torch.stack([pred1, pred2, pred3, pred4, pred5], dim=1)\n",
    "        return pred\n",
    "    \n",
    "model = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch[1/600] loss: 7.226: 100%|██████████| 171/171 [00:01<00:00, 85.53it/s]\n",
      "train epoch[2/600] loss: 6.906: 100%|██████████| 171/171 [00:02<00:00, 67.19it/s]\n",
      "train epoch[3/600] loss: 6.958: 100%|██████████| 171/171 [00:02<00:00, 73.12it/s]\n",
      "train epoch[4/600] loss: 6.965: 100%|██████████| 171/171 [00:01<00:00, 97.05it/s] \n",
      "train epoch[5/600] loss: 6.971: 100%|██████████| 171/171 [00:01<00:00, 102.33it/s]\n",
      "train epoch[6/600] loss: 6.965: 100%|██████████| 171/171 [00:01<00:00, 102.16it/s]\n",
      "train epoch[7/600] loss: 6.952: 100%|██████████| 171/171 [00:01<00:00, 101.86it/s]\n",
      "train epoch[8/600] loss: 6.935: 100%|██████████| 171/171 [00:01<00:00, 91.55it/s]\n",
      "train epoch[9/600] loss: 6.916: 100%|██████████| 171/171 [00:01<00:00, 97.72it/s] \n",
      "train epoch[10/600] loss: 6.897: 100%|██████████| 171/171 [00:01<00:00, 102.54it/s]\n",
      "train epoch[11/600] loss: 6.879: 100%|██████████| 171/171 [00:01<00:00, 102.91it/s]\n",
      "train epoch[12/600] loss: 6.862: 100%|██████████| 171/171 [00:01<00:00, 97.86it/s] \n",
      "train epoch[13/600] loss: 6.850: 100%|██████████| 171/171 [00:01<00:00, 101.90it/s]\n",
      "train epoch[14/600] loss: 6.839: 100%|██████████| 171/171 [00:01<00:00, 101.84it/s]\n",
      "train epoch[15/600] loss: 6.829: 100%|██████████| 171/171 [00:01<00:00, 101.43it/s]\n",
      "train epoch[16/600] loss: 6.818: 100%|██████████| 171/171 [00:01<00:00, 94.09it/s]\n",
      "train epoch[17/600] loss: 6.806: 100%|██████████| 171/171 [00:01<00:00, 101.40it/s]\n",
      "train epoch[18/600] loss: 6.793: 100%|██████████| 171/171 [00:01<00:00, 101.01it/s]\n",
      "train epoch[19/600] loss: 6.778: 100%|██████████| 171/171 [00:01<00:00, 101.26it/s]\n",
      "train epoch[20/600] loss: 6.762: 100%|██████████| 171/171 [00:01<00:00, 95.15it/s] \n",
      "train epoch[21/600] loss: 6.749: 100%|██████████| 171/171 [00:01<00:00, 100.63it/s]\n",
      "train epoch[22/600] loss: 6.740: 100%|██████████| 171/171 [00:01<00:00, 100.96it/s]\n",
      "train epoch[23/600] loss: 6.727: 100%|██████████| 171/171 [00:01<00:00, 100.92it/s]\n",
      "train epoch[24/600] loss: 6.709: 100%|██████████| 171/171 [00:01<00:00, 92.93it/s] \n",
      "train epoch[25/600] loss: 6.693: 100%|██████████| 171/171 [00:01<00:00, 100.63it/s]\n",
      "train epoch[26/600] loss: 6.673: 100%|██████████| 171/171 [00:01<00:00, 100.11it/s]\n",
      "train epoch[27/600] loss: 6.650: 100%|██████████| 171/171 [00:01<00:00, 93.69it/s] \n",
      "train epoch[28/600] loss: 6.631: 100%|██████████| 171/171 [00:01<00:00, 100.18it/s]\n",
      "train epoch[29/600] loss: 6.616: 100%|██████████| 171/171 [00:01<00:00, 100.28it/s]\n",
      "train epoch[30/600] loss: 6.605: 100%|██████████| 171/171 [00:01<00:00, 100.27it/s]\n",
      "train epoch[31/600] loss: 6.600: 100%|██████████| 171/171 [00:01<00:00, 93.34it/s]\n",
      "train epoch[32/600] loss: 6.604: 100%|██████████| 171/171 [00:01<00:00, 100.44it/s]\n",
      "train epoch[33/600] loss: 6.611: 100%|██████████| 171/171 [00:01<00:00, 100.06it/s]\n",
      "train epoch[34/600] loss: 6.613: 100%|██████████| 171/171 [00:01<00:00, 96.05it/s] \n",
      "train epoch[35/600] loss: 6.618: 100%|██████████| 171/171 [00:01<00:00, 99.95it/s] \n",
      "train epoch[36/600] loss: 6.608: 100%|██████████| 171/171 [00:01<00:00, 100.19it/s]\n",
      "train epoch[37/600] loss: 6.617: 100%|██████████| 171/171 [00:01<00:00, 100.21it/s]\n",
      "train epoch[38/600] loss: 6.610: 100%|██████████| 171/171 [00:01<00:00, 95.96it/s]\n",
      "train epoch[39/600] loss: 6.594: 100%|██████████| 171/171 [00:01<00:00, 100.00it/s]\n",
      "train epoch[40/600] loss: 6.588: 100%|██████████| 171/171 [00:01<00:00, 100.51it/s]\n",
      "train epoch[41/600] loss: 6.551: 100%|██████████| 171/171 [00:01<00:00, 95.72it/s] \n",
      "train epoch[42/600] loss: 6.529: 100%|██████████| 171/171 [00:01<00:00, 99.60it/s] \n",
      "train epoch[43/600] loss: 6.534: 100%|██████████| 171/171 [00:01<00:00, 97.95it/s]\n",
      "train epoch[44/600] loss: 6.527: 100%|██████████| 171/171 [00:01<00:00, 99.68it/s] \n",
      "train epoch[45/600] loss: 6.468: 100%|██████████| 171/171 [00:01<00:00, 93.93it/s]\n",
      "train epoch[46/600] loss: 6.406: 100%|██████████| 171/171 [00:01<00:00, 99.84it/s] \n",
      "train epoch[47/600] loss: 6.355: 100%|██████████| 171/171 [00:01<00:00, 88.46it/s]\n",
      "train epoch[48/600] loss: 6.319: 100%|██████████| 171/171 [00:01<00:00, 99.50it/s] \n",
      "train epoch[49/600] loss: 6.273: 100%|██████████| 171/171 [00:01<00:00, 100.97it/s]\n",
      "train epoch[50/600] loss: 6.259: 100%|██████████| 171/171 [00:01<00:00, 92.50it/s]\n",
      "train epoch[51/600] loss: 6.268: 100%|██████████| 171/171 [00:01<00:00, 96.77it/s]\n",
      "train epoch[52/600] loss: 6.261: 100%|██████████| 171/171 [00:01<00:00, 98.85it/s]\n",
      "train epoch[53/600] loss: 6.245: 100%|██████████| 171/171 [00:01<00:00, 94.62it/s]\n",
      "train epoch[54/600] loss: 6.213: 100%|██████████| 171/171 [00:01<00:00, 99.04it/s]\n",
      "train epoch[55/600] loss: 6.215: 100%|██████████| 171/171 [00:01<00:00, 98.48it/s]\n",
      "train epoch[56/600] loss: 6.217: 100%|██████████| 171/171 [00:02<00:00, 84.66it/s]\n",
      "train epoch[57/600] loss: 6.213: 100%|██████████| 171/171 [00:01<00:00, 96.96it/s]\n",
      "train epoch[58/600] loss: 6.184: 100%|██████████| 171/171 [00:02<00:00, 79.40it/s]\n",
      "train epoch[59/600] loss: 6.179: 100%|██████████| 171/171 [00:01<00:00, 87.25it/s]\n",
      "train epoch[60/600] loss: 6.124: 100%|██████████| 171/171 [00:01<00:00, 88.13it/s]\n",
      "train epoch[61/600] loss: 6.109: 100%|██████████| 171/171 [00:01<00:00, 92.57it/s]\n",
      "train epoch[62/600] loss: 6.046: 100%|██████████| 171/171 [00:01<00:00, 96.10it/s]\n",
      "train epoch[63/600] loss: 6.025: 100%|██████████| 171/171 [00:01<00:00, 96.96it/s]\n",
      "train epoch[64/600] loss: 5.992: 100%|██████████| 171/171 [00:01<00:00, 93.18it/s]\n",
      "train epoch[65/600] loss: 5.939: 100%|██████████| 171/171 [00:01<00:00, 94.95it/s]\n",
      "train epoch[66/600] loss: 5.941: 100%|██████████| 171/171 [00:01<00:00, 98.92it/s]\n",
      "train epoch[67/600] loss: 5.907: 100%|██████████| 171/171 [00:01<00:00, 94.95it/s]\n",
      "train epoch[68/600] loss: 5.866: 100%|██████████| 171/171 [00:01<00:00, 96.43it/s]\n",
      "train epoch[69/600] loss: 5.851: 100%|██████████| 171/171 [00:01<00:00, 97.50it/s]\n",
      "train epoch[70/600] loss: 5.871: 100%|██████████| 171/171 [00:01<00:00, 91.45it/s]\n",
      "train epoch[71/600] loss: 6.015: 100%|██████████| 171/171 [00:01<00:00, 96.45it/s]\n",
      "train epoch[72/600] loss: 5.975: 100%|██████████| 171/171 [00:01<00:00, 98.71it/s]\n",
      "train epoch[73/600] loss: 5.847: 100%|██████████| 171/171 [00:01<00:00, 91.22it/s]\n",
      "train epoch[74/600] loss: 5.882: 100%|██████████| 171/171 [00:01<00:00, 95.28it/s]\n",
      "train epoch[75/600] loss: 5.848: 100%|██████████| 171/171 [00:01<00:00, 99.02it/s]\n",
      "train epoch[76/600] loss: 5.805: 100%|██████████| 171/171 [00:01<00:00, 94.77it/s]\n",
      "train epoch[77/600] loss: 5.814: 100%|██████████| 171/171 [00:01<00:00, 98.63it/s]\n",
      "train epoch[78/600] loss: 5.733: 100%|██████████| 171/171 [00:01<00:00, 99.19it/s]\n",
      "train epoch[79/600] loss: 5.754: 100%|██████████| 171/171 [00:01<00:00, 94.20it/s]\n",
      "train epoch[80/600] loss: 5.682: 100%|██████████| 171/171 [00:01<00:00, 95.83it/s]\n",
      "train epoch[81/600] loss: 5.843: 100%|██████████| 171/171 [00:01<00:00, 98.95it/s]\n",
      "train epoch[82/600] loss: 5.762: 100%|██████████| 171/171 [00:01<00:00, 94.16it/s]\n",
      "train epoch[83/600] loss: 5.659: 100%|██████████| 171/171 [00:01<00:00, 98.44it/s]\n",
      "train epoch[84/600] loss: 5.673: 100%|██████████| 171/171 [00:01<00:00, 99.09it/s]\n",
      "train epoch[85/600] loss: 5.663: 100%|██████████| 171/171 [00:01<00:00, 95.05it/s]\n",
      "train epoch[86/600] loss: 5.626: 100%|██████████| 171/171 [00:01<00:00, 98.75it/s]\n",
      "train epoch[87/600] loss: 5.617: 100%|██████████| 171/171 [00:01<00:00, 99.07it/s]\n",
      "train epoch[88/600] loss: 5.613: 100%|██████████| 171/171 [00:01<00:00, 98.86it/s]\n",
      "train epoch[89/600] loss: 5.544: 100%|██████████| 171/171 [00:01<00:00, 95.09it/s]\n",
      "train epoch[90/600] loss: 5.685: 100%|██████████| 171/171 [00:01<00:00, 99.00it/s]\n",
      "train epoch[91/600] loss: 5.568: 100%|██████████| 171/171 [00:01<00:00, 99.16it/s]\n",
      "train epoch[92/600] loss: 5.591: 100%|██████████| 171/171 [00:01<00:00, 90.77it/s]\n",
      "train epoch[93/600] loss: 5.501: 100%|██████████| 171/171 [00:01<00:00, 98.68it/s]\n",
      "train epoch[94/600] loss: 5.531: 100%|██████████| 171/171 [00:01<00:00, 99.31it/s]\n",
      "train epoch[95/600] loss: 5.498: 100%|██████████| 171/171 [00:02<00:00, 72.73it/s]\n",
      "train epoch[96/600] loss: 5.571: 100%|██████████| 171/171 [00:01<00:00, 87.24it/s]\n",
      "train epoch[97/600] loss: 5.487: 100%|██████████| 171/171 [00:01<00:00, 95.63it/s]\n",
      "train epoch[98/600] loss: 5.433: 100%|██████████| 171/171 [00:01<00:00, 92.21it/s]\n",
      "train epoch[99/600] loss: 5.361: 100%|██████████| 171/171 [00:01<00:00, 96.20it/s]\n",
      "train epoch[100/600] loss: 5.435: 100%|██████████| 171/171 [00:01<00:00, 93.41it/s]\n",
      "train epoch[101/600] loss: 5.456: 100%|██████████| 171/171 [00:01<00:00, 97.48it/s]\n",
      "train epoch[102/600] loss: 5.333: 100%|██████████| 171/171 [00:02<00:00, 85.48it/s]\n",
      "train epoch[103/600] loss: 5.305: 100%|██████████| 171/171 [00:01<00:00, 96.66it/s]\n",
      "train epoch[104/600] loss: 5.309: 100%|██████████| 171/171 [00:01<00:00, 87.35it/s]\n",
      "train epoch[105/600] loss: 5.330: 100%|██████████| 171/171 [00:01<00:00, 97.23it/s]\n",
      "train epoch[106/600] loss: 5.343: 100%|██████████| 171/171 [00:01<00:00, 93.34it/s] \n",
      "train epoch[107/600] loss: 5.328: 100%|██████████| 171/171 [00:01<00:00, 97.21it/s] \n",
      "train epoch[108/600] loss: 5.348: 100%|██████████| 171/171 [00:01<00:00, 100.50it/s]\n",
      "train epoch[109/600] loss: 5.328: 100%|██████████| 171/171 [00:01<00:00, 94.75it/s]\n",
      "train epoch[110/600] loss: 5.247: 100%|██████████| 171/171 [00:01<00:00, 96.71it/s]\n",
      "train epoch[111/600] loss: 5.236: 100%|██████████| 171/171 [00:01<00:00, 98.62it/s] \n",
      "train epoch[112/600] loss: 5.298: 100%|██████████| 171/171 [00:02<00:00, 75.34it/s]\n",
      "train epoch[113/600] loss: 5.277: 100%|██████████| 171/171 [00:01<00:00, 97.95it/s] \n",
      "train epoch[114/600] loss: 5.200: 100%|██████████| 171/171 [00:01<00:00, 100.05it/s]\n",
      "train epoch[115/600] loss: 5.216: 100%|██████████| 171/171 [00:01<00:00, 93.47it/s] \n",
      "train epoch[116/600] loss: 5.134: 100%|██████████| 171/171 [00:01<00:00, 100.13it/s]\n",
      "train epoch[117/600] loss: 5.151: 100%|██████████| 171/171 [00:01<00:00, 100.90it/s]\n",
      "train epoch[118/600] loss: 5.087: 100%|██████████| 171/171 [00:01<00:00, 86.21it/s]\n",
      "train epoch[119/600] loss: 5.111: 100%|██████████| 171/171 [00:01<00:00, 90.61it/s]\n",
      "train epoch[120/600] loss: 5.008: 100%|██████████| 171/171 [00:01<00:00, 97.46it/s]\n",
      "train epoch[121/600] loss: 4.995: 100%|██████████| 171/171 [00:01<00:00, 97.64it/s]\n",
      "train epoch[122/600] loss: 5.006: 100%|██████████| 171/171 [00:01<00:00, 92.09it/s]\n",
      "train epoch[123/600] loss: 5.001: 100%|██████████| 171/171 [00:01<00:00, 98.11it/s]\n",
      "train epoch[124/600] loss: 4.947: 100%|██████████| 171/171 [00:01<00:00, 98.10it/s]\n",
      "train epoch[125/600] loss: 4.997: 100%|██████████| 171/171 [00:01<00:00, 85.80it/s]\n",
      "train epoch[126/600] loss: 4.901: 100%|██████████| 171/171 [00:02<00:00, 80.17it/s]\n",
      "train epoch[127/600] loss: 4.875: 100%|██████████| 171/171 [00:01<00:00, 97.09it/s]\n",
      "train epoch[128/600] loss: 4.970: 100%|██████████| 171/171 [00:01<00:00, 91.56it/s]\n",
      "train epoch[129/600] loss: 4.958: 100%|██████████| 171/171 [00:01<00:00, 96.93it/s]\n",
      "train epoch[130/600] loss: 4.932: 100%|██████████| 171/171 [00:01<00:00, 89.20it/s]\n",
      "train epoch[131/600] loss: 4.859: 100%|██████████| 171/171 [00:01<00:00, 99.24it/s] \n",
      "train epoch[132/600] loss: 4.823: 100%|██████████| 171/171 [00:01<00:00, 93.73it/s]\n",
      "train epoch[133/600] loss: 4.814: 100%|██████████| 171/171 [00:01<00:00, 97.51it/s]\n",
      "train epoch[134/600] loss: 4.826: 100%|██████████| 171/171 [00:01<00:00, 92.91it/s]\n",
      "train epoch[135/600] loss: 4.841: 100%|██████████| 171/171 [00:01<00:00, 98.01it/s]\n",
      "train epoch[136/600] loss: 4.775: 100%|██████████| 171/171 [00:01<00:00, 95.09it/s]\n",
      "train epoch[137/600] loss: 4.747: 100%|██████████| 171/171 [00:01<00:00, 95.24it/s]\n",
      "train epoch[138/600] loss: 4.730: 100%|██████████| 171/171 [00:01<00:00, 99.52it/s] \n",
      "train epoch[139/600] loss: 4.657: 100%|██████████| 171/171 [00:01<00:00, 96.09it/s] \n",
      "train epoch[140/600] loss: 4.595: 100%|██████████| 171/171 [00:01<00:00, 97.71it/s]\n",
      "train epoch[141/600] loss: 4.552: 100%|██████████| 171/171 [00:01<00:00, 100.63it/s]\n",
      "train epoch[142/600] loss: 4.602: 100%|██████████| 171/171 [00:01<00:00, 91.28it/s]\n",
      "train epoch[143/600] loss: 4.560: 100%|██████████| 171/171 [00:01<00:00, 96.24it/s]\n",
      "train epoch[144/600] loss: 4.553: 100%|██████████| 171/171 [00:01<00:00, 85.72it/s]\n",
      "train epoch[145/600] loss: 4.553: 100%|██████████| 171/171 [00:01<00:00, 98.44it/s] \n",
      "train epoch[146/600] loss: 4.554: 100%|██████████| 171/171 [00:01<00:00, 94.28it/s] \n",
      "train epoch[147/600] loss: 4.551: 100%|██████████| 171/171 [00:01<00:00, 100.71it/s]\n",
      "train epoch[148/600] loss: 4.553: 100%|██████████| 171/171 [00:01<00:00, 100.65it/s]\n",
      "train epoch[149/600] loss: 4.628: 100%|██████████| 171/171 [00:01<00:00, 96.59it/s]\n",
      "train epoch[150/600] loss: 4.550: 100%|██████████| 171/171 [00:01<00:00, 100.82it/s]\n",
      "train epoch[151/600] loss: 4.535: 100%|██████████| 171/171 [00:01<00:00, 100.27it/s]\n",
      "train epoch[152/600] loss: 4.483: 100%|██████████| 171/171 [00:01<00:00, 95.93it/s]\n",
      "train epoch[153/600] loss: 4.484: 100%|██████████| 171/171 [00:01<00:00, 100.23it/s]\n",
      "train epoch[154/600] loss: 4.524: 100%|██████████| 171/171 [00:01<00:00, 100.21it/s]\n",
      "train epoch[155/600] loss: 4.467: 100%|██████████| 171/171 [00:01<00:00, 93.47it/s]\n",
      "train epoch[156/600] loss: 4.481: 100%|██████████| 171/171 [00:01<00:00, 100.54it/s]\n",
      "train epoch[157/600] loss: 4.455: 100%|██████████| 171/171 [00:01<00:00, 100.67it/s]\n",
      "train epoch[158/600] loss: 4.397: 100%|██████████| 171/171 [00:01<00:00, 92.90it/s] \n",
      "train epoch[159/600] loss: 4.480: 100%|██████████| 171/171 [00:01<00:00, 100.21it/s]\n",
      "train epoch[160/600] loss: 4.460: 100%|██████████| 171/171 [00:01<00:00, 91.59it/s]\n",
      "train epoch[161/600] loss: 4.452: 100%|██████████| 171/171 [00:01<00:00, 100.22it/s]\n",
      "train epoch[162/600] loss: 4.446: 100%|██████████| 171/171 [00:01<00:00, 100.50it/s]\n",
      "train epoch[163/600] loss: 4.423: 100%|██████████| 171/171 [00:01<00:00, 87.76it/s]\n",
      "train epoch[164/600] loss: 4.478: 100%|██████████| 171/171 [00:01<00:00, 95.43it/s]\n",
      "train epoch[165/600] loss: 4.428: 100%|██████████| 171/171 [00:01<00:00, 88.53it/s]\n",
      "train epoch[166/600] loss: 4.410: 100%|██████████| 171/171 [00:01<00:00, 96.94it/s]\n",
      "train epoch[167/600] loss: 4.438: 100%|██████████| 171/171 [00:01<00:00, 97.03it/s]\n",
      "train epoch[168/600] loss: 4.450: 100%|██████████| 171/171 [00:01<00:00, 92.13it/s]\n",
      "train epoch[169/600] loss: 4.416: 100%|██████████| 171/171 [00:01<00:00, 97.91it/s]\n",
      "train epoch[170/600] loss: 4.480: 100%|██████████| 171/171 [00:01<00:00, 90.27it/s]\n",
      "train epoch[171/600] loss: 4.410: 100%|██████████| 171/171 [00:02<00:00, 84.07it/s]\n",
      "train epoch[172/600] loss: 4.381: 100%|██████████| 171/171 [00:01<00:00, 92.24it/s]\n",
      "train epoch[173/600] loss: 4.386: 100%|██████████| 171/171 [00:01<00:00, 90.75it/s]\n",
      "train epoch[174/600] loss: 4.385: 100%|██████████| 171/171 [00:01<00:00, 96.08it/s]\n",
      "train epoch[175/600] loss: 4.407: 100%|██████████| 171/171 [00:01<00:00, 94.18it/s]\n",
      "train epoch[176/600] loss: 4.439: 100%|██████████| 171/171 [00:01<00:00, 93.76it/s]\n",
      "train epoch[177/600] loss: 4.449: 100%|██████████| 171/171 [00:01<00:00, 99.73it/s] \n",
      "train epoch[178/600] loss: 4.421: 100%|██████████| 171/171 [00:02<00:00, 79.84it/s]\n",
      "train epoch[179/600] loss: 4.436: 100%|██████████| 171/171 [00:01<00:00, 94.42it/s]\n",
      "train epoch[180/600] loss: 4.384: 100%|██████████| 171/171 [00:01<00:00, 92.39it/s]\n",
      "train epoch[181/600] loss: 4.430: 100%|██████████| 171/171 [00:01<00:00, 96.96it/s]\n",
      "train epoch[182/600] loss: 4.348: 100%|██████████| 171/171 [00:02<00:00, 76.29it/s]\n",
      "train epoch[183/600] loss: 4.376: 100%|██████████| 171/171 [00:01<00:00, 90.97it/s]\n",
      "train epoch[184/600] loss: 4.407: 100%|██████████| 171/171 [00:01<00:00, 96.62it/s]\n",
      "train epoch[185/600] loss: 4.386: 100%|██████████| 171/171 [00:02<00:00, 83.41it/s]\n",
      "train epoch[186/600] loss: 4.368: 100%|██████████| 171/171 [00:01<00:00, 99.71it/s]\n",
      "train epoch[187/600] loss: 4.330: 100%|██████████| 171/171 [00:01<00:00, 97.63it/s]\n",
      "train epoch[188/600] loss: 4.331: 100%|██████████| 171/171 [00:01<00:00, 85.54it/s]\n",
      "train epoch[189/600] loss: 4.311: 100%|██████████| 171/171 [00:01<00:00, 95.08it/s]\n",
      "train epoch[190/600] loss: 4.356: 100%|██████████| 171/171 [00:01<00:00, 91.58it/s]\n",
      "train epoch[191/600] loss: 4.355: 100%|██████████| 171/171 [00:01<00:00, 97.67it/s] \n",
      "train epoch[192/600] loss: 4.365: 100%|██████████| 171/171 [00:01<00:00, 93.12it/s]\n",
      "train epoch[193/600] loss: 4.305: 100%|██████████| 171/171 [00:01<00:00, 100.51it/s]\n",
      "train epoch[194/600] loss: 4.356: 100%|██████████| 171/171 [00:01<00:00, 95.93it/s] \n",
      "train epoch[195/600] loss: 4.307: 100%|██████████| 171/171 [00:01<00:00, 90.36it/s]\n",
      "train epoch[196/600] loss: 4.432: 100%|██████████| 171/171 [00:01<00:00, 97.22it/s]\n",
      "train epoch[197/600] loss: 4.382: 100%|██████████| 171/171 [00:01<00:00, 92.29it/s]\n",
      "train epoch[198/600] loss: 4.306: 100%|██████████| 171/171 [00:01<00:00, 95.24it/s]\n",
      "train epoch[199/600] loss: 4.279: 100%|██████████| 171/171 [00:01<00:00, 90.13it/s]\n",
      "train epoch[200/600] loss: 4.264: 100%|██████████| 171/171 [00:01<00:00, 99.70it/s]\n",
      "train epoch[201/600] loss: 4.293: 100%|██████████| 171/171 [00:01<00:00, 90.55it/s]\n",
      "train epoch[202/600] loss: 4.258: 100%|██████████| 171/171 [00:01<00:00, 93.98it/s]\n",
      "train epoch[203/600] loss: 4.252: 100%|██████████| 171/171 [00:01<00:00, 98.28it/s]\n",
      "train epoch[204/600] loss: 4.347: 100%|██████████| 171/171 [00:01<00:00, 89.57it/s]\n",
      "train epoch[205/600] loss: 4.256: 100%|██████████| 171/171 [00:01<00:00, 98.58it/s]\n",
      "train epoch[206/600] loss: 4.233: 100%|██████████| 171/171 [00:01<00:00, 90.72it/s]\n",
      "train epoch[207/600] loss: 4.249: 100%|██████████| 171/171 [00:01<00:00, 98.52it/s]\n",
      "train epoch[208/600] loss: 4.228: 100%|██████████| 171/171 [00:01<00:00, 95.07it/s]\n",
      "train epoch[209/600] loss: 4.260: 100%|██████████| 171/171 [00:01<00:00, 93.58it/s]\n",
      "train epoch[210/600] loss: 4.248: 100%|██████████| 171/171 [00:01<00:00, 98.57it/s]\n",
      "train epoch[211/600] loss: 4.263: 100%|██████████| 171/171 [00:01<00:00, 87.57it/s]\n",
      "train epoch[212/600] loss: 4.209: 100%|██████████| 171/171 [00:01<00:00, 96.79it/s]\n",
      "train epoch[213/600] loss: 4.237: 100%|██████████| 171/171 [00:01<00:00, 97.12it/s]\n",
      "train epoch[214/600] loss: 4.242: 100%|██████████| 171/171 [00:01<00:00, 94.03it/s]\n",
      "train epoch[215/600] loss: 4.218: 100%|██████████| 171/171 [00:01<00:00, 92.28it/s]\n",
      "train epoch[216/600] loss: 4.219: 100%|██████████| 171/171 [00:01<00:00, 96.59it/s]\n",
      "train epoch[217/600] loss: 4.232: 100%|██████████| 171/171 [00:01<00:00, 97.93it/s]\n",
      "train epoch[218/600] loss: 4.239: 100%|██████████| 171/171 [00:01<00:00, 93.82it/s]\n",
      "train epoch[219/600] loss: 4.230: 100%|██████████| 171/171 [00:01<00:00, 98.21it/s]\n",
      "train epoch[220/600] loss: 4.305: 100%|██████████| 171/171 [00:01<00:00, 91.81it/s]\n",
      "train epoch[221/600] loss: 4.205: 100%|██████████| 171/171 [00:01<00:00, 98.04it/s]\n",
      "train epoch[222/600] loss: 4.258: 100%|██████████| 171/171 [00:01<00:00, 98.35it/s]\n",
      "train epoch[223/600] loss: 4.315: 100%|██████████| 171/171 [00:01<00:00, 90.54it/s]\n",
      "train epoch[224/600] loss: 4.187: 100%|██████████| 171/171 [00:01<00:00, 98.41it/s]\n",
      "train epoch[225/600] loss: 4.248: 100%|██████████| 171/171 [00:01<00:00, 91.10it/s]\n",
      "train epoch[226/600] loss: 4.345: 100%|██████████| 171/171 [00:01<00:00, 98.24it/s]\n",
      "train epoch[227/600] loss: 4.198: 100%|██████████| 171/171 [00:01<00:00, 94.55it/s]\n",
      "train epoch[228/600] loss: 4.210: 100%|██████████| 171/171 [00:01<00:00, 98.57it/s]\n",
      "train epoch[229/600] loss: 4.164: 100%|██████████| 171/171 [00:01<00:00, 98.04it/s]\n",
      "train epoch[230/600] loss: 4.183: 100%|██████████| 171/171 [00:01<00:00, 90.03it/s]\n",
      "train epoch[231/600] loss: 4.204: 100%|██████████| 171/171 [00:01<00:00, 97.90it/s]\n",
      "train epoch[232/600] loss: 4.206: 100%|██████████| 171/171 [00:01<00:00, 92.00it/s]\n",
      "train epoch[233/600] loss: 4.202: 100%|██████████| 171/171 [00:01<00:00, 98.36it/s]\n",
      "train epoch[234/600] loss: 4.202: 100%|██████████| 171/171 [00:01<00:00, 98.20it/s]\n",
      "train epoch[235/600] loss: 4.152: 100%|██████████| 171/171 [00:01<00:00, 94.11it/s]\n",
      "train epoch[236/600] loss: 4.156: 100%|██████████| 171/171 [00:01<00:00, 98.19it/s]\n",
      "train epoch[237/600] loss: 4.199: 100%|██████████| 171/171 [00:01<00:00, 91.16it/s]\n",
      "train epoch[238/600] loss: 4.149: 100%|██████████| 171/171 [00:01<00:00, 98.61it/s]\n",
      "train epoch[239/600] loss: 4.163: 100%|██████████| 171/171 [00:01<00:00, 92.80it/s]\n",
      "train epoch[240/600] loss: 4.230: 100%|██████████| 171/171 [00:01<00:00, 98.45it/s]\n",
      "train epoch[241/600] loss: 4.183: 100%|██████████| 171/171 [00:01<00:00, 93.79it/s]\n",
      "train epoch[242/600] loss: 4.171: 100%|██████████| 171/171 [00:01<00:00, 98.70it/s]\n",
      "train epoch[243/600] loss: 4.179: 100%|██████████| 171/171 [00:01<00:00, 98.65it/s]\n",
      "train epoch[244/600] loss: 4.219: 100%|██████████| 171/171 [00:01<00:00, 93.56it/s]\n",
      "train epoch[245/600] loss: 4.143: 100%|██████████| 171/171 [00:01<00:00, 96.41it/s]\n",
      "train epoch[246/600] loss: 4.191: 100%|██████████| 171/171 [00:01<00:00, 94.18it/s]\n",
      "train epoch[247/600] loss: 4.160: 100%|██████████| 171/171 [00:01<00:00, 97.64it/s]\n",
      "train epoch[248/600] loss: 4.141: 100%|██████████| 171/171 [00:01<00:00, 94.72it/s]\n",
      "train epoch[249/600] loss: 4.261: 100%|██████████| 171/171 [00:01<00:00, 97.98it/s]\n",
      "train epoch[250/600] loss: 4.222: 100%|██████████| 171/171 [00:01<00:00, 96.43it/s]\n",
      "train epoch[251/600] loss: 4.212: 100%|██████████| 171/171 [00:01<00:00, 93.80it/s]\n",
      "train epoch[252/600] loss: 4.172: 100%|██████████| 171/171 [00:01<00:00, 98.14it/s]\n",
      "train epoch[253/600] loss: 4.176: 100%|██████████| 171/171 [00:02<00:00, 77.93it/s]\n",
      "train epoch[254/600] loss: 4.177: 100%|██████████| 171/171 [00:01<00:00, 97.13it/s]\n",
      "train epoch[255/600] loss: 4.127: 100%|██████████| 171/171 [00:01<00:00, 93.57it/s]\n",
      "train epoch[256/600] loss: 4.191: 100%|██████████| 171/171 [00:01<00:00, 98.39it/s]\n",
      "train epoch[257/600] loss: 4.072: 100%|██████████| 171/171 [00:01<00:00, 93.96it/s]\n",
      "train epoch[258/600] loss: 4.215: 100%|██████████| 171/171 [00:01<00:00, 98.56it/s]\n",
      "train epoch[259/600] loss: 4.155: 100%|██████████| 171/171 [00:01<00:00, 98.38it/s]\n",
      "train epoch[260/600] loss: 4.054: 100%|██████████| 171/171 [00:01<00:00, 91.38it/s]\n",
      "train epoch[261/600] loss: 4.083: 100%|██████████| 171/171 [00:01<00:00, 98.06it/s]\n",
      "train epoch[262/600] loss: 4.243: 100%|██████████| 171/171 [00:01<00:00, 89.85it/s]\n",
      "train epoch[263/600] loss: 4.071: 100%|██████████| 171/171 [00:01<00:00, 98.19it/s]\n",
      "train epoch[264/600] loss: 4.109: 100%|██████████| 171/171 [00:01<00:00, 94.14it/s]\n",
      "train epoch[265/600] loss: 4.116: 100%|██████████| 171/171 [00:01<00:00, 98.25it/s]\n",
      "train epoch[266/600] loss: 4.188: 100%|██████████| 171/171 [00:01<00:00, 94.57it/s]\n",
      "train epoch[267/600] loss: 4.110: 100%|██████████| 171/171 [00:01<00:00, 97.88it/s]\n",
      "train epoch[268/600] loss: 4.086: 100%|██████████| 171/171 [00:01<00:00, 98.18it/s]\n",
      "train epoch[269/600] loss: 4.210: 100%|██████████| 171/171 [00:01<00:00, 93.40it/s]\n",
      "train epoch[270/600] loss: 4.071: 100%|██████████| 171/171 [00:01<00:00, 97.69it/s]\n",
      "train epoch[271/600] loss: 4.097: 100%|██████████| 171/171 [00:02<00:00, 65.67it/s]\n",
      "train epoch[272/600] loss: 4.043: 100%|██████████| 171/171 [00:01<00:00, 96.43it/s]\n",
      "train epoch[273/600] loss: 4.027: 100%|██████████| 171/171 [00:01<00:00, 90.64it/s]\n",
      "train epoch[274/600] loss: 4.038: 100%|██████████| 171/171 [00:01<00:00, 97.91it/s]\n",
      "train epoch[275/600] loss: 4.085: 100%|██████████| 171/171 [00:01<00:00, 92.19it/s]\n",
      "train epoch[276/600] loss: 4.044: 100%|██████████| 171/171 [00:01<00:00, 96.60it/s]\n",
      "train epoch[277/600] loss: 4.388:   8%|▊         | 14/171 [00:00<00:01, 89.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m loss2 \u001b[38;5;241m+\u001b[39m loss3 \u001b[38;5;241m+\u001b[39m loss4 \u001b[38;5;241m+\u001b[39m loss5\n\u001b[0;32m---> 23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"loss =\", loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for data in train_bar:\n",
    "        x_train, y_train = data\n",
    "        optimizer.zero_grad()\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        loss1 = loss_function(y_train_pred[:, 0, :], y_train[:, 0])\n",
    "        loss2 = loss_function(y_train_pred[:, 1, :], y_train[:, 1])\n",
    "        loss3 = loss_function(y_train_pred[:, 2, :], y_train[:, 2])\n",
    "        loss4 = loss_function(y_train_pred[:, 3, :], y_train[:, 3])\n",
    "        loss5 = loss_function(y_train_pred[:, 4, :], y_train[:, 4])\n",
    "        \n",
    "        if config.loss_type == 1:\n",
    "            loss = max(loss1, loss2, loss3, loss4, loss5)\n",
    "        if config.loss_type == 2:\n",
    "            loss = loss1 + loss2 + loss3 + loss4 + loss5\n",
    "        if config.loss_type == 3:\n",
    "            loss = 5 * loss1 + 2 * loss2 + loss3 + loss4 + loss5\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"loss =\", loss)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_bar.desc = 'train epoch[{}/{}] loss: {:.3f}'.format(epoch + 1, config.epochs, loss)\n",
    "    \n",
    "    # model.eval()\n",
    "    # test_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     test_bar = tqdm(test_loader)\n",
    "    #     for data in test_bar:\n",
    "    #         x_test, y_test = data\n",
    "    #         y_test_pred = model(x_test)\n",
    "    #         loss1 = loss_function(y_test_pred[:, 0, :], y_test[:, 0])\n",
    "    #         loss2 = loss_function(y_test_pred[:, 1, :], y_test[:, 1])\n",
    "    #         loss3 = loss_function(y_test_pred[:, 2, :], y_test[:, 2])\n",
    "    #         loss4 = loss_function(y_test_pred[:, 3, :], y_test[:, 3])\n",
    "    #         loss5 = loss_function(y_test_pred[:, 4, :], y_test[:, 4])\n",
    "    #         # test_loss = max(loss1, loss2, loss3, loss4, loss5)\n",
    "    #         # test_loss = loss1 + loss2 + loss3 + loss4 + loss5\n",
    "    #         test_loss = 5 * loss1 + 3 * loss2 + loss3 + loss4 + loss5\n",
    "    \n",
    "    # if test_loss < config.best_loss:\n",
    "    #     config.best_loss = test_loss\n",
    "    #     torch.save(model.state_dict(), config.save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:03<00:00, 48.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set predict rate for step 1 is: 0.794630\n",
      "train set predict rate for step 2 is: 0.829729\n",
      "train set predict rate for step 3 is: 0.839626\n",
      "train set predict rate for step 4 is: 0.840359\n",
      "train set predict rate for step 5 is: 0.804435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 51.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set predict rate for step 1 is: 0.377109\n",
      "test set predict rate for step 2 is: 0.342260\n",
      "test set predict rate for step 3 is: 0.320249\n",
      "test set predict rate for step 4 is: 0.307777\n",
      "test set predict rate for step 5 is: 0.296405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "# model.load_state_dict(torch.load(config.save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_bar = tqdm(train_loader)\n",
    "\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    for data in train_bar:\n",
    "        x_train, y_train = data\n",
    "        y_train_pred = model(x_train)\n",
    "        for i in range(config.predict_len):\n",
    "            tot[i] += x_train.shape[0]\n",
    "        for i in range(x_train.shape[0]):\n",
    "            for j in range(config.predict_len):\n",
    "                pred = 0\n",
    "                for k in range(config.output_size):\n",
    "                    if y_train_pred[i][j][k].item() > y_train_pred[i][j][pred].item():\n",
    "                        pred = k\n",
    "                predict_rate[j] += (1 if pred == y_train[i][j] else 0)\n",
    "    for i in range(config.predict_len):\n",
    "        print('train set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "    test_bar = tqdm(test_loader)\n",
    "\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    for data in test_bar:\n",
    "        x_test, y_test = data\n",
    "        y_test_pred = model(x_test)\n",
    "        for i in range(config.predict_len):\n",
    "            tot[i] += x_test.shape[0]\n",
    "        for i in range(x_test.shape[0]):\n",
    "            for j in range(config.predict_len):\n",
    "                pred = 0\n",
    "                for k in range(config.output_size):\n",
    "                    if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                        pred = k\n",
    "                predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "    for i in range(config.predict_len):\n",
    "        print('test set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if config.loss_type == 1:\n",
    "    save_path = './trained_models/{}_max.pth'.format(config.model_name)\n",
    "if config.loss_type == 2:\n",
    "    save_path = './trained_models/{}_sum.pth'.format(config.model_name)\n",
    "if config.loss_type == 3:\n",
    "    save_path = './trained_models/{}_lc.pth'.format(config.model_name)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------model_sum------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:05<00:00, 29.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set predict rate for step 1 is: 0.794630\n",
      "train set predict rate for step 2 is: 0.829729\n",
      "train set predict rate for step 3 is: 0.839626\n",
      "train set predict rate for step 4 is: 0.840359\n",
      "train set predict rate for step 5 is: 0.804435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:01<00:00, 35.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set predict rate for step 1 is: 0.377109\n",
      "test set predict rate for step 2 is: 0.342260\n",
      "test set predict rate for step 3 is: 0.320249\n",
      "test set predict rate for step 4 is: 0.307777\n",
      "test set predict rate for step 5 is: 0.296405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load model and predict, then save the result\n",
    "def compute_prediction(model, output_file):\n",
    "    model.eval()\n",
    "    \n",
    "    lst = []\n",
    "    column = ['is_train_set', 'pred_y1', 'pred_y2', 'pred_y3', 'pred_y4', 'pred_y5', 'target_y1', 'target_y2', 'target_y3', 'target_y4', 'target_y5']\n",
    "    with torch.no_grad():\n",
    "        train_bar = tqdm(train_loader)\n",
    "\n",
    "        predict_rate = [0 for _ in range(config.predict_len)]\n",
    "        tot = [0 for _ in range(config.predict_len)]\n",
    "        for data in train_bar:\n",
    "            x_train, y_train = data\n",
    "            y_train_pred = model(x_train)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_train.shape[0]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                row = [1]\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_train_pred[i][j][k].item() > y_train_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_train[i][j] else 0)\n",
    "                    row.append(pred)\n",
    "                for j in range(config.predict_len):\n",
    "                    row.append(y_train[i][j].item())\n",
    "                lst.append(row)\n",
    "        for i in range(config.predict_len):\n",
    "            print('train set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "        \n",
    "        test_bar = tqdm(test_loader)\n",
    "\n",
    "        predict_rate = [0 for _ in range(config.predict_len)]\n",
    "        tot = [0 for _ in range(config.predict_len)]\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            y_test_pred = model(x_test)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_test.shape[0]\n",
    "            for i in range(x_test.shape[0]):\n",
    "                row = [0]\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "                    row.append(pred)\n",
    "                for j in range(config.predict_len):\n",
    "                    row.append(y_test[i][j].item())\n",
    "                lst.append(row)\n",
    "        for i in range(config.predict_len):\n",
    "            print('test set predict rate for step {:d} is: {:f}'.format(i + 1, predict_rate[i] / tot[i]))\n",
    "    df = pd.DataFrame(lst, columns = column)\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "config.hidden_size = 60\n",
    "model_max = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "model_sum = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "model_lc = GRURNN(config.feature_size, config.hidden_size, config.num_layers, config.output_size)\n",
    "#model_max.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_max.pth'))\n",
    "model_sum.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_sum_overfit.pth'))\n",
    "#model_lc.load_state_dict(torch.load('./trained_models/GRU_for_tennis_momentum_swing_lc.pth'))\n",
    "# print('-----------model_max------------')\n",
    "# compute_prediction(model_max, './pred_res/model_max.csv')\n",
    "print('-----------model_sum------------')\n",
    "compute_prediction(model_sum, './pred_res/model_sum_overfit.csv')\n",
    "# print('----model_linear_combination----')\n",
    "# compute_prediction(model_lc, './pred_res/model_lc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:01<00:00, 39.94it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 48.34it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.94it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.84it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.07it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.35it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.71it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.38it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.34it/s]\n",
      "100%|██████████| 43/43 [00:01<00:00, 36.80it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 45.44it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.52it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "def calcPredictRate(model, test_loader):\n",
    "    model.eval()\n",
    "    predict_rate = [0 for _ in range(config.predict_len)]\n",
    "    tot = [0 for _ in range(config.predict_len)]\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_bar = tqdm(test_loader)\n",
    "\n",
    "        for data in test_bar:\n",
    "            x_test, y_test = data\n",
    "            y_test_pred = model(x_test)\n",
    "            for i in range(config.predict_len):\n",
    "                tot[i] += x_test.shape[0]\n",
    "            for i in range(x_test.shape[0]):\n",
    "                for j in range(config.predict_len):\n",
    "                    pred = 0\n",
    "                    for k in range(config.output_size):\n",
    "                        if y_test_pred[i][j][k].item() > y_test_pred[i][j][pred].item():\n",
    "                            pred = k\n",
    "                    predict_rate[j] += (1 if pred == y_test[i][j] else 0)\n",
    "    return np.array([predict_rate[i] / tot[i] for i in range(config.predict_len)])\n",
    "\n",
    "def calcFeatureImportance(model):\n",
    "    accuracy = calcPredictRate(model, test_loader)\n",
    "    importance = np.zeros(config.feature_size)\n",
    "    for i in range(config.feature_size):\n",
    "        ntestX = testX.copy()\n",
    "        nfeature = ntestX[:, :, i].copy().reshape(-1)\n",
    "        np.random.shuffle(nfeature)\n",
    "        nfeature = nfeature.reshape(-1, config.timestep)\n",
    "        for j in range(ntestX.shape[0]):\n",
    "            for k in range(ntestX.shape[1]):\n",
    "                ntestX[j][k][i] = nfeature[j][k]\n",
    "        nx_test_tensor = torch.from_numpy(ntestX).to(torch.float32)\n",
    "        ntest_data = TensorDataset(nx_test_tensor, y_test_tensor)\n",
    "        ntest_loader = DataLoader(ntest_data, config.batch_size, False)\n",
    "        naccuracy = calcPredictRate(model, ntest_loader)\n",
    "        importance[i] = sum([(accuracy[j] - naccuracy[j]) * (accuracy[j] - naccuracy[j]) for j in range(config.predict_len)])\n",
    "\n",
    "    tot = np.sum(importance)\n",
    "    for i in range(importance.shape[0]):\n",
    "        importance[i] = importance[i] / tot\n",
    "    \n",
    "    return importance\n",
    "\n",
    "column_name = ['model_name'] + ['f' + str(i + 1) for i in range(config.feature_size)]\n",
    "z = [['model_sum'] + calcFeatureImportance(model_sum).tolist()]\n",
    "#z = [['model_max'] + calcFeatureImportance(model_max).tolist(), ['model_sum'] + calcFeatureImportance(model_sum).tolist(), ['model_lc'] + calcFeatureImportance(model_lc).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['model_sum', 0.021650982446062568, 0.004823531399415421, 0.03488823936880161, 0.4939600263562488, 0.02715031509233138, 0.07847741979083948, 0.07525046883711511, 0.012113737349845435, 0.11641521228606662, 0.03588504620791024, 0.05512003919647231, 0.04426498166889114]]\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "importance = pd.DataFrame(data=z, columns=column_name)\n",
    "importance.to_csv('./trained_models/feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
