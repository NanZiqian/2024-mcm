\section{Swing Prediction Model}

\subsection{Model Introduction}~{}

To predict the swings, we need to find potential factors that may affect future momentum.
In problem 1, we established a standard to judge the player's performance;
in problem 2, we found the player's performance, ``momentum'', has a one order of autocorrelation,
which indicates that some factors of the momentum have impact on the future momentum.

To find what and how factors affect the future momentum, we can take advantage 
of Machine Learning. In this section, we will primarily focus on establishing a 
supervised learning classification model using the Gated Recurrent Unit (GRU) network to 
predict momentum swings. We will break down the problems into three parts:

\begin{itemize}
    \item definition of the swing of the play
    \item data (potential factors) and label used to train GRU
    \item Utilizing Gated Recurrent Unit
\end{itemize}

\subsubsection{Definition of ``Swings of the Play''}~{}

We first give the definition of the swings of the play.
Based on our previous definition of ``momentum,'' the significant changes of the game 
largely depend on the ``momentum'' of the two players. Therefore, we choose ``momentum'' to represent 
the swings of the play. The specific definition is as follows:\\

We use $\Delta f(t)$ to represent the difference in ``momentum'' between the two players., 
so it can be easily seen that if $\Delta f(t)$ and $\Delta f(t+1)$
have different signs, it indicates a ``swing'' in the game's momentum. 
In this way, we can define four states of ``momentum'' at time $t$:

$$states=\begin{cases}
    state1, &\text{if } \Delta f(t)>0 \text{ and } \Delta f(t+1)>0, \text{ which means stay positive}\\
    state2, &\text{if } \Delta f(t)<0 \text{ and } \Delta f(t+1)>0, \text{ which means rise from negative to positive}\\
    state3, &\text{if } \Delta f(t)<0 \text{ and } \Delta f(t+1)<0, \text{ which means stay negative}\\
    state4, &\text{if } \Delta f(t)>0 \text{ and } \Delta f(t+1)<0, \text{ which means decrease from positive to negative}
\end{cases}$$
Obviously, if state 2 or state 4 appears, we can determine that a swing has occurred. 

To visualize the swing, we choose four shapes of lines to represent the four states of swings at time $t$:

\begin{figure}[H]
    \centering
        \includegraphics[width=\linewidth]{mainmatter/imgs/states.jpg}
    \caption{visualization of swings, from state1 to state4}
    \label{fig:states}
\end{figure}

Here we display the momentum swings drived from AHP of first three matches.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_match1.png}
        \caption{swings in match 1}
    \end{subfigure}\hspace{-0.02\textwidth}
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_match2.png}
        \caption{swings in match 2}
    \end{subfigure}\hspace{-0.02\textwidth}
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_match3.png}
        \caption{swings in match 3}
    \end{subfigure}
    \caption{swings in first three matches}
    \label{fig:swings in matches}
\end{figure}


\subsubsection{Data and Label used to train GRU}~{}

To verify all potential factors, meanwhile avoiding over-fitting due to the large number of features,
the data's feature we select is \dots

What's more, we want to find out can swings merely be predicted by the technical data,
so that we can give advice to players on skills,
therefore we delete \verb|scored_last_point, score_diff, game_victor, set_victor| before we train a second model.

The label we use is the state of the swing derived from model of AHP.

we use different sets of data to train and derive different models.

\begin{itemize}
    \item First, we will train the model with data from all matches available.
    \item Then, we will train a new model with data from a single player.
    \item Finally, we will train the model with part of available match statistics
\end{itemize}

From the first model, we can find the important factors that affect all players' momentum swings.
From the second model, we can find the important factors that affect 
especially well on that player's momentum swings.
From the third model, we can examine
the generalization ability to accurately predict the swings in irrelevant players and matches.

\subsubsection{Utilizing the Gated Recurrent Unit (GRU) Network}~{}

Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) 
architecture that has gained popularity for sequential data processing. 
And it is appropriate for our many(10)-to-many(5) prediction model.

We will focus on a single unit of recurrent neural network to interpret the hidden mathematical principles:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.15]{mainmatter/imgs/6.jpg}
    \caption{Structure of GRU}
\end{figure}

\begin{enumerate}
    \item 
    The update gate at time step $t$ is computed using the following formula:  
    $$  z_t = \sigma(W_z \cdot x_t + U_z \cdot h_{t-1})  $$  
    Here, when $x_t$ is input to the network unit, 
    it is multiplied by its own weight $W_z$. 
    Similarly, $h_{t-1}$, which holds the information from the previous $t-1$ units, 
    is multiplied by its own weight $U_z$. These two results are then summed together 
    and passed through a sigmoid activation function to compress the result between 0 and 1. 
  
    \item 
    The essence of the reset gate is for the model to decide how much past information to forget. To compute it, we use:  
    $$  r_t = \sigma(W_r \cdot x_t + U_r \cdot h_{t-1})  $$  

    \item 
    The computation of the new memory content using the reset gate is as follows:  
    $$ h_t'= tanh(Wx_t+r_t\odot Uh_{t-1})$$

    \item 
    The computation for the new memory content $h_t$ using the update gate is as follows:  
    $$ h_t=z_t\odot h_{t-1}+(1-z_t)\odot h_t'$$ 
\end{enumerate}

So, we can predict the momentum swings from the technical data of the past 10 points.

\subsection{Visualization and Analysis}~{}

Out of simplicity, the model shown in this section is trained merely on technical data,
and only the next swing of the next 5 swings is shown.

First, we display the model trained on all matches available to 
predict the swings of the play on test data.
To avoid dazzling figure, we minus the swing from AHP with prediction, showing the difference.

Here's the difference of first three matches. It's worth mentioning that the training data
is fisrt 0.8 of the match.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_diff_match1_overfit.png}
        \caption{difference in match 1}
    \end{subfigure}\hspace{-0.02\textwidth}
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_diff_match2_overfit.png}
        \caption{difference in match 2}
    \end{subfigure}\hspace{-0.02\textwidth}
    \begin{subfigure}[b]{0.34\textwidth}
        \includegraphics[width=\linewidth]{mainmatter/imgs/swing_diff_match3_overfit.png}
        \caption{difference in match 3}
    \end{subfigure}
    \caption{difference between prediction and AHP model}
    \label{fig:diff swing}
\end{figure}

Together with other predictions can be concluded as the table below, 
if we calculate the prediction rate by the number of correct predictions (the flat part of the figure)
divided by the total number of points:

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Step} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
        \midrule
        \textbf{Train Set} & 0.7946 & 0.8297 & 0.8396 & 0.8404 & 0.8044 \\
        \textbf{Test Set} & 0.3771 & 0.3423 & 0.3203 & 0.3078 & 0.2964 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy Scores for Each Step}
    \label{tab:predict_rate_untrained}
\end{table}

To make sure training the model is useful, we give the prediction rate of untrained model.

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Step} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
        \midrule
        \textbf{Train Set} & 0.1827 & 0.1821 & 0.1949 & 0.3174 & 0.1816 \\
        \textbf{Test Set} & 0.1768 & 0.1768 & 0.1915 & 0.3199 & 0.1724 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy Scores of Untrained Model}
    \label{tab:predict_rate_untrained}
\end{table}


We can see that the prediction rate of the untrained model is around 0.25,
namely the probability of random guess. 
The prediction rate of the trained model is much higher.

Considering the situation of over-fitting, we decrease the epochs and result is as follows:

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Step} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
        \midrule
        \textbf{Train Set} & 0.5379 & 0.4930 & 0.4693 & 0.4579 & 0.4471 \\
        \textbf{Test Set} & 0.5073 & 0.4274 & 0.4057 & 0.4094 & 0.4035 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy Scores of Fit Model}
    \label{tab:predict_rate_fit}
\end{table}

Now we can see that the prediction rate of the trained model is bigger than random guess,
we can conclude that momentum swings can be predicted by the data 
(technical data if only consider the statistics shown) of the past 10 points.


\subsubsection{Prediction Error compared with AHP model outcome}~{}

Although the prediction rate is bigger than random guess, 0.5 is not a big number.
Now we analyze the reason.

\paragraph{Factors we may fail to consider}~{}

First of all, the AHP model is not considering all possible factors to momentum.
For example, the.
So the overlap rate's effectiveness is limited.

Then, from the conclusion of problem 2, momentum only has one order of autocorrelation.
So when we try to predict the far future momentum, there's no reason to be accurate.

\subsection{Weight of Factors}~{}

If we can derive the derivative of output with respect to input, we can get the weight of factors.
Although it's difficult to directly acquire derivative, we can use Permutation Feature Importance theory
to avoid getting the derivative.

\subsubsection{Permutation Feature Importance theory}~{}

Permutation Feature Importance theory is used to measure the importance of a feature by calculating 
the increase in the model's prediction error after permuting the feature. A feature is “important” if shuffling its values increases the model error, 
because in this case the model relied on the feature for the prediction. A feature is “unimportant” if 
shuffling its values leaves the model error unchanged, because in this case the model ignored the feature 
for the prediction. 

\begin{algorithm}[H]
    \caption{Permutation Feature Importance}  
    \textbf{Input:} Trained model $\hat{f}$, feature matrix $X$, target vector $y$, error measure $L(y, \hat{f})$.  
      
    Estimate the original model error $e_{orig} = L(y, \hat{f}(X))$ (e.g. mean squared error)  
      
    \textbf{For} each feature $j \in \{1, ..., p\}$ \textbf{do:}  
      
    \quad\quad $\circ$ Generate feature matrix $X_{perm}$ by permuting feature $j$ in the data $X$. This breaks the association between feature $j$ and true outcome $y$.  
      
    \quad\quad $\circ$ Estimate error $e_{perm} = L(Y, \hat{f}(X_{perm}))$ based on the predictions of the permuted data.  
      
    \quad\quad $\circ$ Calculate permutation feature importance as quotient $FI_j = e_{perm} / e_{orig}$ or difference $FI_j = e_{perm} - e_{orig}$  
      
    Sort features by descending FI.
\end{algorithm}

Using the model trained on all matches available, and the above argorithm,
we sort FI and normalize them, then we plot the figure:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{mainmatter/imgs/8.png}
    \caption{feature importance}
\end{figure}

We can see that the most important feature is fast win (rally count $<$ 3 and wins the point),
and the second most important feature is unforced error. Compared to problem 1 (main skill features are
running distance, unforced error and fast win), they coincide. The running distance become insignificant, 
we think it is because this factor is also related to fast win.

The two factors are the most important factors that affect all players.

Using the model with recent score data, we plot the figure:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{mainmatter/imgs/weight_with_score.jpg}
    \caption{feature importance}
\end{figure}

We can find out that the most important feature is the score difference,
way more important than technical features. 
This is because the recent score condition is the most direct factor to the momentum.
Now we have expanded our knowledge to problem 2, that among multiple factors
that affect future momentum, 
the recent score condition is the most important.

\subsection{Advice against Opponent Based on Weight of Factors}~{}

As said previously, model trained from match data of a single player,
the factor with bigger weight is extremely effective to this player.

And to give specific advice against the opponent, we choose the model trained on technical data.

We choose three players:rublev(three matches), sinner(four matches), alcaraz(five matches), and 
we use the feature analysis method in problem 3 to find their features, the result is as follows:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{mainmatter/imgs/9.png}
    \caption{features of three players}
\end{figure}
\begin{enumerate}
    \item In comparison to general features, both Rublev and Alcaraz phave significant feature fast win. 
    Therefore, when competing against other players, we recommend them to emphasize the rhythm of seizing 
    the early shots. For example, they can focus more on the quality of their serves, 
    progressively achieving a fast win to enhance their momentum. 
    \item Additionally, Rublev is hardly affected by negative factors such as double fault and 
    unforced error. Hence, in matches, there is no need for him to worry about occasional mistakes; 
    instead, he can play boldly.
    \item For Sinner, his technical characteristics are markedly different. 
    Important features for him are winners and return depth. Consequently, 
    we advise Sinner to fully exploit his strong scoring abilities of winner points
    and utilize his control over return depth to steer the course of the match and establish an advantage.

\end{enumerate}

Now we have finished Problem 3.